{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One run full walktrhough "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Do the full walk through on the large data set\n",
    "* Refactor the source code and bring it to individual scripts\n",
    "* Ensure a full run with one click"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your base path is at: code_Covid_19_data_analysis'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## check some parameters\n",
    "## depending where you launch your notebook, the relative path might not work\n",
    "## you should start the notebook server from your base path\n",
    "## when opening the notebook, typically your path will be ../ads_covid-19/notebooks\n",
    "import os\n",
    "if os.path.split(os.getcwd())[-1]=='notebooks':\n",
    "    os.chdir(\"../\")\n",
    "\n",
    "'Your base path is at: '+os.path.split(os.getcwd())[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Update all data\n",
    "\n",
    "### GDP prompt  clean it\n",
    "\n",
    "#### task 1: Clean up code\n",
    "\n",
    "Clean up code and document the following code. Use pep-8 guidelines and the possibility to use doxygen for automated code documentation. ''' ...code ...'''\n",
    "\n",
    "#### task 2: ask for explanation: \n",
    "\n",
    "what is this command: subprocess.Popen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: Already up to date.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %load src/data/get_data.py\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def get_johns_hopkins():\n",
    "    \"\"\"\n",
    "    Retrieve data using a git pull request.\n",
    "    \n",
    "    The source code must be pulled first. The result is stored in the predefined CSV structure.\n",
    "    \n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    git_pull_command = \"/usr/bin/git pull\"\n",
    "    covid_data_directory = \"../COVID-19/\"  # take care for the correct path\n",
    "    \n",
    "    git_pull = subprocess.Popen(\n",
    "        git_pull_command,\n",
    "        cwd=os.path.dirname(covid_data_directory),\n",
    "        shell=True,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE\n",
    "    )\n",
    "    \n",
    "    out, error = git_pull.communicate()\n",
    "\n",
    "    if error:\n",
    "        print(f\"Error: {error.decode('utf-8')}\")\n",
    "    \n",
    "    if out:\n",
    "        print(f\"Output: {out.decode('utf-8')}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    get_johns_hopkins()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Process pipeline \n",
    "\n",
    "### GDP prompt  clean it\n",
    "\n",
    "#### task 3: Simplify code and document it \n",
    "\n",
    "please write this code in a simpler form and explain the changes.\n",
    "Try to use less commands and clearer documentation\n",
    "\n",
    "''' ... your code ...'''\n",
    "\n",
    "#### output for changes\n",
    "\n",
    "changes made:\n",
    "\n",
    "* Shortened the function name to store_relational_jh_data to follow the PEP-8 guidelines for lowercase function names with words separated by underscores.\n",
    "* Combined the column renaming and filling missing state values into one line using the fillna function with a dictionary.\n",
    "* Replaced the chained operations for transforming the dataset into a relational model with the melt function, which is more concise and easier to understand.\n",
    "* Used f-strings for printing the output, which makes the code cleaner and more readable.\n",
    "* Added comments to explain each step of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows stored: 330327\n",
      "Latest date is: 2023-03-09 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# %load src/data/process_JH_data.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def store_relational_JH_data():\n",
    "    \"\"\"\n",
    "    Transform the COVID-19 data into a relational dataset and save it as a CSV file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Read the raw data from the CSV file\n",
    "    data_path = '../COVID-19/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv'\n",
    "    data = pd.read_csv(data_path)\n",
    "\n",
    "    # Rename columns and fill missing state values with 'no'\n",
    "    data = data.rename(columns={'Country/Region': 'country', 'Province/State': 'state'}).fillna({'state': 'no'})\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    data = data.drop(['Lat', 'Long'], axis=1)\n",
    "\n",
    "    # Transform the dataset into a relational model\n",
    "    relational_data = data.melt(id_vars=['state', 'country'], var_name='date', value_name='confirmed')\n",
    "    relational_data['date'] = pd.to_datetime(relational_data['date'])\n",
    "\n",
    "    # Save the relational model to a CSV file\n",
    "    output_file = 'data/processed/COVID_relational_confirmed.csv'\n",
    "    relational_data.to_csv(output_file, sep=';', index=False)\n",
    "\n",
    "    print(f'Number of rows stored: {relational_data.shape[0]}')\n",
    "    print(f'Latest date is: {relational_data[\"date\"].max()}')\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "\n",
    "    store_relational_JH_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strip down to a smaller file\n",
    "\n",
    "attention we are overwriting the existing file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the smaller file\n",
    "countries_to_extract = ['US','Germany', 'India','France'] # add your country of interest\n",
    "\n",
    "pd_result_larg =pd.read_csv('data/processed/COVID_relational_confirmed.csv', sep=';', parse_dates=[0])\n",
    "pd_result_larg = pd_result_larg[pd_result_larg['country'].isin(countries_to_extract)]\n",
    "\n",
    "# Save the relational model to a CSV file\n",
    "output_file = 'data/processed/COVID_relational_confirmed.csv'\n",
    "pd_result_larg.to_csv(output_file, sep=';', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3  Filter and Doubling Rate Calculation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### GDP prompt  to accelerate the function\n",
    "\n",
    "####  task 4: help for profiling\n",
    "\n",
    "How can I profile the code. my goal is to understand where I need most of the time. Each function should be listet how long it took  \n",
    "\n",
    "####  task 5: use an other library (even a wrong librarary stated)\n",
    "\n",
    "the code is not faster use arrow to accelarte it\n",
    "\n",
    "\n",
    "####  task 6: iterative search for error\n",
    "\n",
    "this code does not work, where is the error. the error message is below as well ... copy error and code ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the test slope is: [2.]\n",
      "Time taken by 'get_doubling_time_via_regression': 0.00 seconds\n",
      "Time taken to read and sort data: 0.03 seconds\n",
      "Time taken by 'calc_filtered_data': 0.04 seconds\n",
      "Time taken by 'calc_doubling_rate': 7.20 seconds\n",
      "Time taken by 'calc_doubling_rate' with 'confirmed_filtered': 7.16 seconds\n",
      "Time taken to process and save the final dataset: 0.09 seconds\n",
      "      state  country        date  confirmed  confirmed_filtered  confirmed_DR  \\\n",
      "10647    no  Germany  2021-12-31    7109182           7100628.0    331.771117   \n",
      "10662    no  Germany  2022-01-01    7109182           7109182.0           inf   \n",
      "10677    no  Germany  2022-01-02    7109182           7135027.2           inf   \n",
      "10692    no  Germany  2022-01-03    7109182           7172654.8           inf   \n",
      "10707    no  Germany  2022-01-04    7238408           7223150.4    110.693782   \n",
      "\n",
      "       confirmed_filtered_DR  \n",
      "10647             379.348284  \n",
      "10662             565.200582  \n",
      "10677             413.669256  \n",
      "10692             224.945320  \n",
      "10707             162.884329  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "reg = linear_model.LinearRegression(fit_intercept=True)\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import dask.dataframe as dd\n",
    "\n",
    "from scipy import signal\n",
    "\n",
    "\n",
    "def get_doubling_time_via_regression(in_array):\n",
    "    ''' Use a linear regression to approximate the doubling rate\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        in_array : pandas.series\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        Doubling rate: double\n",
    "    '''\n",
    "\n",
    "    y = np.array(in_array)\n",
    "    X = np.arange(-1,2).reshape(-1, 1)\n",
    "\n",
    "    assert len(in_array)==3\n",
    "    reg.fit(X,y)\n",
    "    intercept=reg.intercept_\n",
    "    slope=reg.coef_\n",
    "\n",
    "    return intercept/slope\n",
    "\n",
    "\n",
    "def savgol_filter(df_input,column='confirmed',window=5):\n",
    "    ''' Savgol Filter which can be used in groupby apply function (data structure kept)\n",
    "\n",
    "        parameters:\n",
    "        ----------\n",
    "        df_input : pandas.series\n",
    "        column : str\n",
    "        window : int\n",
    "            used data points to calculate the filter result\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        df_result: pd.DataFrame\n",
    "            the index of the df_input has to be preserved in result\n",
    "    '''\n",
    "\n",
    "    degree=1\n",
    "    df_result=df_input\n",
    "\n",
    "    filter_in=df_input[column].fillna(0) # attention with the neutral element here\n",
    "\n",
    "    result=signal.savgol_filter(np.array(filter_in),\n",
    "                           window, # window size used for filtering\n",
    "                           1)\n",
    "    df_result[str(column+'_filtered')]=result\n",
    "    return df_result\n",
    "\n",
    "def rolling_reg(df_input,col='confirmed'):\n",
    "    ''' Rolling Regression to approximate the doubling time'\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        df_input: pd.DataFrame\n",
    "        col: str\n",
    "            defines the used column\n",
    "        Returns:\n",
    "        ----------\n",
    "        result: pd.DataFrame\n",
    "    '''\n",
    "    days_back=3\n",
    "    result=df_input[col].rolling(\n",
    "                window=days_back,\n",
    "                min_periods=days_back).apply(get_doubling_time_via_regression,raw=False)\n",
    "\n",
    "\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calc_filtered_data(df_input,filter_on='confirmed'):\n",
    "    '''  Calculate savgol filter and return merged data frame\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        df_input: pd.DataFrame\n",
    "        filter_on: str\n",
    "            defines the used column\n",
    "        Returns:\n",
    "        ----------\n",
    "        df_output: pd.DataFrame\n",
    "            the result will be joined as a new column on the input data frame\n",
    "    '''\n",
    "\n",
    "    must_contain=set(['state','country',filter_on])\n",
    "    assert must_contain.issubset(set(df_input.columns)), ' Erro in calc_filtered_data not all columns in data frame'\n",
    "\n",
    "    df_output=df_input.copy() # we need a copy here otherwise the filter_on column will be overwritten\n",
    "\n",
    "    pd_filtered_result=df_output[['state','country',filter_on]].groupby(['state','country']).apply(savgol_filter)#.reset_index()\n",
    "\n",
    "    #print('--+++ after group by apply')\n",
    "    #print(pd_filtered_result[pd_filtered_result['country']=='Germany'].tail())\n",
    "\n",
    "    #df_output=pd.merge(df_output,pd_filtered_result[['index',str(filter_on+'_filtered')]],on=['index'],how='left')\n",
    "    df_output=pd.merge(df_output,pd_filtered_result[[str(filter_on+'_filtered')]],left_index=True,right_index=True,how='left')\n",
    "    #print(df_output[df_output['country']=='Germany'].tail())\n",
    "    return df_output.copy()\n",
    "\n",
    "\n",
    "    \n",
    "def calc_doubling_rate(df_input, filter_on='confirmed'):\n",
    "    \"\"\"Calculate approximated doubling rate and return merged data frame.\"\"\"\n",
    "\n",
    "    # Convert the input DataFrame to a Dask DataFrame\n",
    "    dask_input = dd.from_pandas(df_input, npartitions=8)  # You can adjust the number of partitions based on your system's resources.\n",
    "\n",
    "    # Define a custom function for Dask's apply method\n",
    "    def calculate_doubling_rate(df):\n",
    "        df[filter_on + '_DR'] = df[filter_on].rolling(window=3, min_periods=3).apply(get_doubling_time_via_regression, raw=False)\n",
    "        return df\n",
    "\n",
    "    # Calculate the doubling rate for each group\n",
    "    meta = df_input.copy()\n",
    "    meta[filter_on + '_DR'] = np.nan\n",
    "    dask_result = dask_input.groupby(['state', 'country']).apply(calculate_doubling_rate, meta=meta)\n",
    "\n",
    "    # Convert the result back to a pandas DataFrame\n",
    "    df_output = dask_result.compute()\n",
    "\n",
    "    return df_output\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    test_data_reg = np.array([2, 4, 6])\n",
    "    result = get_doubling_time_via_regression(test_data_reg)\n",
    "    print('the test slope is: ' + str(result))\n",
    "    print(f\"Time taken by 'get_doubling_time_via_regression': {time.time() - start_time:.2f} seconds\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    pd_JH_data = pd.read_csv('data/processed/COVID_relational_confirmed.csv', sep=';', parse_dates=[0])\n",
    "    pd_JH_data = pd_JH_data.sort_values('date', ascending=True).copy()\n",
    "    print(f\"Time taken to read and sort data: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    pd_result_larg = calc_filtered_data(pd_JH_data)\n",
    "    print(f\"Time taken by 'calc_filtered_data': {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    pd_result_larg = calc_doubling_rate(pd_result_larg)\n",
    "    print(f\"Time taken by 'calc_doubling_rate': {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    pd_result_larg = calc_doubling_rate(pd_result_larg, 'confirmed_filtered')\n",
    "    print(f\"Time taken by 'calc_doubling_rate' with 'confirmed_filtered': {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    mask = pd_result_larg['confirmed'] > 100\n",
    "    pd_result_larg['confirmed_filtered_DR'] = pd_result_larg['confirmed_filtered_DR'].where(mask, other=np.NaN)\n",
    "    pd_result_larg.to_csv('data/processed/COVID_final_set.csv', sep=';', index=False)\n",
    "    print(f\"Time taken to process and save the final dataset: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    print(pd_result_larg[pd_result_larg['country'] == 'Germany'].tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3b  Test function\n",
    "\n",
    "#### GDP prompt  to accelerate the function\n",
    "\n",
    "##### task 7: write a test function\n",
    "\n",
    "please write a test function for the following function to check for correct results\n",
    "''' code ''''\n",
    "\n",
    "##### output: \n",
    "attention the result was not correct, always cross check correct results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def test_get_doubling_time_via_regression():\n",
    "    reg = LinearRegression(fit_intercept=True)\n",
    "\n",
    "    # Test case 1: Doubling rate = 2\n",
    "    test_data_1 = np.array([1, 2, 4])\n",
    "    expected_result_1 = 1.55555556\n",
    "    result_1 = get_doubling_time_via_regression(test_data_1)\n",
    "    assert np.isclose(result_1, expected_result_1, rtol=1e-05), f\"Expected {expected_result_1}, but got {result_1}\"\n",
    "\n",
    "    # Test case 2: Doubling rate = 3\n",
    "    test_data_2 = np.array([4, 8, 16])\n",
    "    expected_result_2 = 1.55555556\n",
    "    result_2 = get_doubling_time_via_regression(test_data_2)\n",
    "    assert np.isclose(result_2, expected_result_2, rtol=1e-05), f\"Expected {expected_result_2}, but got {result_2}\"\n",
    "\n",
    "    # Test case 3: Doubling rate = 1\n",
    "    test_data_3 = np.array([1, 2, 4])\n",
    "    expected_result_3 = 1.55555556\n",
    "    result_3 = get_doubling_time_via_regression(test_data_3)\n",
    "    assert np.isclose(result_3, expected_result_3, rtol=1e-05), f\"Expected {expected_result_3}, but got {result_3}\"\n",
    "\n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "test_get_doubling_time_via_regression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      state country        date  confirmed  confirmed_filtered  confirmed_DR  \\\n",
      "10649    no      US  2021-12-31   54907717          54719451.0     97.959334   \n",
      "10664    no      US  2022-01-01   55099948          55247702.6    155.624410   \n",
      "10679    no      US  2022-01-02   55396191          55822126.6    225.742286   \n",
      "10694    no      US  2022-01-03   56438983          56429298.4     83.112153   \n",
      "10709    no      US  2022-01-04   57267794          57166945.2     60.234629   \n",
      "\n",
      "       confirmed_filtered_DR  \n",
      "10649             126.403018  \n",
      "10664             115.255426  \n",
      "10679             100.234545  \n",
      "10694              94.504470  \n",
      "10709              83.985736  \n"
     ]
    }
   ],
   "source": [
    "print(pd_result_larg[pd_result_larg['country']=='US'].tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Visual Board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kienlef/Documents/Vorlesung/Covid19_update_2023/code_Covid_19_data_analysis\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: on\n"
     ]
    }
   ],
   "source": [
    "# %load src/visualization/visualize.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import dash\n",
    "dash.__version__\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output,State\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import os\n",
    "print(os.getcwd())\n",
    "df_input_large=pd.read_csv('data/processed/COVID_final_set.csv',sep=';')\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "app = dash.Dash()\n",
    "app.layout = html.Div([\n",
    "\n",
    "    dcc.Markdown('''\n",
    "    #  Applied Data Science on COVID-19 data\n",
    "\n",
    "    Goal of the project is to teach data science by applying a cross industry standard process,\n",
    "    it covers the full walkthrough of: automated data gathering, data transformations,\n",
    "    filtering and machine learning to approximating the doubling time, and\n",
    "    (static) deployment of responsive dashboard.\n",
    "\n",
    "    '''),\n",
    "\n",
    "    dcc.Markdown('''\n",
    "    ## Multi-Select Country for visualization\n",
    "    '''),\n",
    "\n",
    "\n",
    "    dcc.Dropdown(\n",
    "        id='country_drop_down',\n",
    "        options=[ {'label': each,'value':each} for each in df_input_large['country'].unique()],\n",
    "        value=['US', 'Germany','Italy'], # which are pre-selected\n",
    "        multi=True\n",
    "    ),\n",
    "\n",
    "    dcc.Markdown('''\n",
    "        ## Select Timeline of confirmed COVID-19 cases or the approximated doubling time\n",
    "        '''),\n",
    "\n",
    "\n",
    "    dcc.Dropdown(\n",
    "    id='doubling_time',\n",
    "    options=[\n",
    "        {'label': 'Timeline Confirmed ', 'value': 'confirmed'},\n",
    "        {'label': 'Timeline Confirmed Filtered', 'value': 'confirmed_filtered'},\n",
    "        {'label': 'Timeline Doubling Rate', 'value': 'confirmed_DR'},\n",
    "        {'label': 'Timeline Doubling Rate Filtered', 'value': 'confirmed_filtered_DR'},\n",
    "    ],\n",
    "    value='confirmed',\n",
    "    multi=False\n",
    "    ),\n",
    "\n",
    "    dcc.Graph(figure=fig, id='main_window_slope')\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "@app.callback(\n",
    "    Output('main_window_slope', 'figure'),\n",
    "    [Input('country_drop_down', 'value'),\n",
    "    Input('doubling_time', 'value')])\n",
    "def update_figure(country_list,show_doubling):\n",
    "\n",
    "\n",
    "    if 'doubling_rate' in show_doubling:\n",
    "        my_yaxis={'type':\"log\",\n",
    "               'title':'Approximated doubling rate over 3 days (larger numbers are better #stayathome)'\n",
    "              }\n",
    "    else:\n",
    "        my_yaxis={'type':\"log\",\n",
    "                  'title':'Confirmed infected people (source johns hopkins csse, log-scale)'\n",
    "              }\n",
    "\n",
    "\n",
    "    traces = []\n",
    "    for each in country_list:\n",
    "\n",
    "        df_plot=df_input_large[df_input_large['country']==each]\n",
    "\n",
    "        if show_doubling=='doubling_rate_filtered':\n",
    "            df_plot=df_plot[['state','country','confirmed','confirmed_filtered','confirmed_DR','confirmed_filtered_DR','date']].groupby(['country','date']).agg(np.mean).reset_index()\n",
    "        else:\n",
    "            df_plot=df_plot[['state','country','confirmed','confirmed_filtered','confirmed_DR','confirmed_filtered_DR','date']].groupby(['country','date']).agg(np.sum).reset_index()\n",
    "       #print(show_doubling)\n",
    "\n",
    "\n",
    "        traces.append(dict(x=df_plot.date,\n",
    "                                y=df_plot[show_doubling],\n",
    "                                mode='markers+lines',\n",
    "                                opacity=0.9,\n",
    "                                name=each\n",
    "                        )\n",
    "                )\n",
    "\n",
    "    return {\n",
    "            'data': traces,\n",
    "            'layout': dict (\n",
    "                width=1280,\n",
    "                height=720,\n",
    "\n",
    "                xaxis={'title':'Timeline',\n",
    "                        'tickangle':-45,\n",
    "                        'nticks':20,\n",
    "                        'tickfont':dict(size=14,color=\"#7f7f7f\"),\n",
    "                      },\n",
    "\n",
    "                yaxis=my_yaxis\n",
    "        )\n",
    "    }\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    app.run_server(debug=True, use_reloader=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
